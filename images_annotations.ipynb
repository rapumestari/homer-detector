{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7c90968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory structure created.\n",
      "Annotations loaded:\n",
      "  homer_simpson: 612 annotations\n",
      "  principal_skinner: 506 annotations\n",
      "  marge_simpson: 557 annotations\n",
      "  lisa_simpson: 562 annotations\n",
      "  bart_simpson: 554 annotations\n",
      "  chief_wiggum: 209 annotations\n",
      "\n",
      "Selected 1000 total annotations\n",
      "  Homer: 500\n",
      "  principal_skinner: 100\n",
      "  marge_simpson: 100\n",
      "  lisa_simpson: 100\n",
      "  bart_simpson: 100\n",
      "  chief_wiggum: 100\n",
      "\n",
      "All annotations verified successfully!\n",
      "\n",
      "FINAL SUMMARY ------------\n",
      "Homer images: 500\n",
      "Non-Homer images (per character): 100\n",
      "Total non-Homer images: 500\n",
      "Total images: 1000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "# You will need to download the dataset from: https://www.kaggle.com/datasets/alexattia/the-simpsons-characters-dataset \n",
    "# make sure it's in the correct folder structure as expected by the script.\n",
    "\n",
    "BASE_PATH = \"homer/simpsons_dataset\"\n",
    "ANNOTATION_FILE = \"homer/annotation.txt\"\n",
    "OUTPUT_PATH = \"homer/project\"\n",
    "\n",
    "CHARACTERS = {\n",
    "    \"homer_simpson\": 0,\n",
    "    \"principal_skinner\": 1,\n",
    "    \"marge_simpson\": 1,\n",
    "    \"lisa_simpson\": 1,\n",
    "    \"bart_simpson\": 1,\n",
    "    \"chief_wiggum\": 1\n",
    "}\n",
    "\n",
    "HOMER_TARGET = 500\n",
    "NON_HOMER_PER_CLASS = 100\n",
    "TRAIN_RATIO = 0.8\n",
    "\n",
    "# Create directory structure\n",
    "for split in [\"train\", \"val\"]:\n",
    "    os.makedirs(os.path.join(OUTPUT_PATH, \"images\", split), exist_ok=True)\n",
    "    os.makedirs(os.path.join(OUTPUT_PATH, \"labels\", split), exist_ok=True)\n",
    "print(\"Directory structure created.\")\n",
    "\n",
    "# Load annotations - Group by character\n",
    "annotations_by_character = {char: [] for char in CHARACTERS.keys()}\n",
    "\n",
    "with open(ANNOTATION_FILE, \"r\") as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(\",\")\n",
    "        if len(parts) < 6:\n",
    "            continue\n",
    "        img_path = parts[0]  # Keep original path\n",
    "        x1, y1, x2, y2 = map(int, parts[1:5])\n",
    "        cls_name = parts[5]\n",
    "        \n",
    "        # Only keep annotations for characters we care about\n",
    "        if cls_name in CHARACTERS:\n",
    "            # Store: (image_path, x1, y1, x2, y2)\n",
    "            annotations_by_character[cls_name].append((img_path, x1, y1, x2, y2))\n",
    "\n",
    "print(f\"Annotations loaded:\")\n",
    "for char, annots in annotations_by_character.items():\n",
    "    print(f\"  {char}: {len(annots)} annotations\")\n",
    "\n",
    "# Helper: YOLO conversion\n",
    "def to_yolo(x1, y1, x2, y2, w_img, h_img):\n",
    "    w = x2 - x1\n",
    "    h = y2 - y1\n",
    "    cx = x1 + w / 2\n",
    "    cy = y1 + h / 2\n",
    "    return cx / w_img, cy / h_img, w / w_img, h / h_img\n",
    "\n",
    "selected_annotations = []\n",
    "\n",
    "# Homer annotations\n",
    "homer_annots = annotations_by_character[\"homer_simpson\"]\n",
    "if len(homer_annots) < HOMER_TARGET:\n",
    "    raise ValueError(f\"Not enough Homer annotations. Found {len(homer_annots)}, need {HOMER_TARGET}\")\n",
    "selected_homer = random.sample(homer_annots, HOMER_TARGET)\n",
    "selected_annotations.extend([(\"homer_simpson\", ann) for ann in selected_homer])\n",
    "\n",
    "# Non-Homer annotations\n",
    "for char in [\"principal_skinner\", \"marge_simpson\", \"lisa_simpson\", \"bart_simpson\", \"chief_wiggum\"]:\n",
    "    char_annots = annotations_by_character[char]\n",
    "    if len(char_annots) < NON_HOMER_PER_CLASS:\n",
    "        raise ValueError(f\"Not enough {char} annotations. Found {len(char_annots)}, need {NON_HOMER_PER_CLASS}\")\n",
    "    selected = random.sample(char_annots, NON_HOMER_PER_CLASS)\n",
    "    selected_annotations.extend([(char, ann) for ann in selected])\n",
    "\n",
    "# Shuffle all selected annotations\n",
    "random.shuffle(selected_annotations)\n",
    "\n",
    "print(f\"\\nSelected {len(selected_annotations)} total annotations\")\n",
    "print(f\"  Homer: {len(selected_homer)}\")\n",
    "for char in [\"principal_skinner\", \"marge_simpson\", \"lisa_simpson\", \"bart_simpson\", \"chief_wiggum\"]:\n",
    "    count = sum(1 for c, _ in selected_annotations if c == char)\n",
    "    print(f\"  {char}: {count}\")\n",
    "\n",
    "# Verify annotations\n",
    "verification_passed = True\n",
    "\n",
    "for char_name, (img_path, x1, y1, x2, y2) in selected_annotations:\n",
    "    # Convert annotation path to actual file path\n",
    "    actual_path = img_path.replace(\"./characters/\", BASE_PATH + \"/\")\n",
    "    \n",
    "    if not os.path.exists(actual_path):\n",
    "        print(f\"Image not found: {actual_path}\")\n",
    "        verification_passed = False\n",
    "if verification_passed:\n",
    "    print(\"\\nAll annotations verified successfully!\")\n",
    "else:\n",
    "    print(\"Verification failed. Check errors above.\")\n",
    "\n",
    "# Split into train/val & save\n",
    "split_idx = int(len(selected_annotations) * TRAIN_RATIO)\n",
    "train_annotations = selected_annotations[:split_idx]\n",
    "val_annotations = selected_annotations[split_idx:]\n",
    "\n",
    "# Group annotations by image for processing\n",
    "def process_split(annotation_list, split_name):\n",
    "    # Group by image path\n",
    "    images_dict = {}\n",
    "    for char_name, (img_path, x1, y1, x2, y2) in annotation_list:\n",
    "        if img_path not in images_dict:\n",
    "            images_dict[img_path] = []\n",
    "        images_dict[img_path].append((char_name, x1, y1, x2, y2))\n",
    "    \n",
    "    # Track image numbers per character\n",
    "    character_counters = {char: 1 for char in CHARACTERS.keys()}\n",
    "    \n",
    "    saved_images = 0\n",
    "    for img_path, annots in images_dict.items():\n",
    "        # Convert path\n",
    "        src_img = img_path.replace(\"./characters/\", BASE_PATH + \"/\")\n",
    "        \n",
    "        # Get primary character (first annotation for this image)\n",
    "        primary_char = annots[0][0]\n",
    "        \n",
    "        # Get file extension\n",
    "        ext = os.path.splitext(img_path)[1]\n",
    "        \n",
    "        # Create new filename with character name and number\n",
    "        new_filename = f\"{primary_char}_{character_counters[primary_char]:04d}{ext}\"\n",
    "        character_counters[primary_char] += 1\n",
    "        \n",
    "        dst_img = os.path.join(OUTPUT_PATH, \"images\", split_name, new_filename)\n",
    "        \n",
    "        # Copy image\n",
    "        shutil.copy2(src_img, dst_img)\n",
    "        \n",
    "        # Read image dimensions\n",
    "        img = cv2.imread(src_img)\n",
    "        h_img, w_img = img.shape[:2]\n",
    "        \n",
    "        # Create YOLO label file\n",
    "        label_filename = os.path.splitext(new_filename)[0] + \".txt\"\n",
    "        label_path = os.path.join(OUTPUT_PATH, \"labels\", split_name, label_filename)\n",
    "        \n",
    "        # Write all annotations for this image\n",
    "        with open(label_path, \"w\") as f:\n",
    "            for char_name, x1, y1, x2, y2 in annots:\n",
    "                class_id = CHARACTERS[char_name]\n",
    "                x_center, y_center, width, height = to_yolo(x1, y1, x2, y2, w_img, h_img)\n",
    "                f.write(f\"{class_id} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\\n\")\n",
    "        \n",
    "        saved_images += 1\n",
    "\n",
    "process_split(train_annotations, \"train\")\n",
    "process_split(val_annotations, \"val\")\n",
    "\n",
    "print(\"\\nFINAL SUMMARY ------------\")\n",
    "print(f\"Homer images: {HOMER_TARGET}\")\n",
    "print(f\"Non-Homer images (per character): {NON_HOMER_PER_CLASS}\")\n",
    "print(f\"Total non-Homer images: {NON_HOMER_PER_CLASS * 5}\")\n",
    "print(f\"Total images: {HOMER_TARGET + NON_HOMER_PER_CLASS * 5}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
